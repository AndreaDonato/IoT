Parametri: max_r1=40, max_r2=25, add_r1_max=5, add_r2_max=3, out_r1_max=3, out_r2_max=2, low_th=15, med_th=30, gamma=0.95, steps=1000, seed=12345, simulations=10, iterations=155, tot_states=2132
Q-learning: episodes=10000, steps_per_ep=1000, alpha=0.100, eps_start=0.20, eps_end=0.02, eps_decay=0.995, avg_return~-489.64
[VI] s0=(0,0,TL1G), steps=1000, reward=-450
[VI] s0=(0,0,TL1G), steps=1000, reward=-427
[VI] s0=(0,0,TL1G), steps=1000, reward=-454
[VI] s0=(0,0,TL1G), steps=1000, reward=-404
[VI] s0=(0,0,TL1G), steps=1000, reward=-467
[VI] s0=(0,0,TL1G), steps=1000, reward=-471
[VI] s0=(0,0,TL1G), steps=1000, reward=-393
[VI] s0=(0,0,TL1G), steps=1000, reward=-378
[VI] s0=(0,0,TL1G), steps=1000, reward=-438
[VI] s0=(0,0,TL1G), steps=1000, reward=-407
[QL] s0=(0,0,TL1G), steps=1000, reward=-364
[QL] s0=(0,0,TL1G), steps=1000, reward=-414
[QL] s0=(0,0,TL1G), steps=1000, reward=-458
[QL] s0=(0,0,TL1G), steps=1000, reward=-430
[QL] s0=(0,0,TL1G), steps=1000, reward=-358
[QL] s0=(0,0,TL1G), steps=1000, reward=-447
[QL] s0=(0,0,TL1G), steps=1000, reward=-414
[QL] s0=(0,0,TL1G), steps=1000, reward=-374
[QL] s0=(0,0,TL1G), steps=1000, reward=-419
[QL] s0=(0,0,TL1G), steps=1000, reward=-340
